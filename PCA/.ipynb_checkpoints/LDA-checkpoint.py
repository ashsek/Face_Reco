
'''
Always check for TODOs before rapping up

Dimentionality reduction and classification using LDA
# =============================================================================

strategy:
    get line (direction) with highest eigen value after implementing LDA
    fully
    project all points or only mean of each class on line
    
    mean is not rational if Knn : if K is > 1
    
    so  we will project test point on line now dimentions is much reduced and 
    can measure distance with all training points and find label accordingly
    
current step:
    projecting training and test sets separately using same projection
    getting 39 dominant eigen vectors instead of just one (done)
    eigen values and vectors and dominant one (done by ny tested)
    within class scatter matrix S (done with doubt I think can go wrong need to implement it in the other way too)
    center class matrices Zi (done)
    finding Sb (replacement to B for higher no of classes) in LDA  (done)
    finding mean for each class training data (done)
# =============================================================================
'''
import numpy as np
#number of images in total
rows=400
#differenct people
number_of_classes=40
nImages_in_each_class=10
#dimentions of vector (image) before reduction
dimentions=10304

#making dummy matrix to play with
#D=np.random.rand(400,10304)
#TODO repalce this with real D
D=np.ones((rows, dimentions))
#print(D)

#z=np.zeros((1, dimentions))
#making an array to save means of each class (40,10304)
classes_means=np.zeros((number_of_classes,dimentions))
for i in range(number_of_classes):
    for j in range (nImages_in_each_class):
        #adding evens because index of arrays start at zero not 1
        if(j % 2 == 0):
            classes_means[i]+=D[i*10+j][:]
classes_means/=(nImages_in_each_class/2)
    

#finding Sb (replacement to B for higher no of classes) in LDA
#nk is number of samples in kth class
nk=nImages_in_each_class // 2 
#overall sample mean
#get mean of each column and result is 1 row and 10304(dimentions) columns
meu=np.mean(D,axis=0)
#print("meu:")
#print("shape of meu :",meu.shape)  #(10304,)
#initializing Sb
Sb=np.zeros((dimentions,dimentions))

for k in range(number_of_classes):
    diff_means=classes_means[k]-meu
    diff_means=np.reshape(diff_means,(1,10304) )
    dm_t=diff_means.transpose()
    B=np.matmul(dm_t,diff_means)
    Sb+=nk*B
#print(Sb.shape) #(10304, 10304)
"""center class matrices Zi i=0,1,2...39 : """
Z=D
for i in range(number_of_classes):
    for j in range (nImages_in_each_class):
        #TODO classes means for dummy data is array of 5s is that correct?
        if(j % 2 == 0):
            #print(classes_means[i])
            #initializing S : within-class scatter matrix
            #TODO check what are real dimentions of S and initialize it accordingly
            #S=Sb=np.zeros((1,dimentions))
            #class scatter matrices Si
            Z[i*10+j]-=classes_means[i][:]
#print(Z)
#print("shape of Z:",Z.shape) #(400, 10304)

  
"""within class scatter matrix S :"""
          
#TODO check if Si the within class scatter matrix is same as covarience matrix
#of Z center class matrix
# or is it equal to covarience matrix of class i mean and so on
#or should I calculate S as in Algorithm
#S_i=np.cov(classes_means[i]) #S+=S_i #in for loop ?
#or : #S=np.cov(Z.T) #produces 10304 * 10304 matrix
#print(S.shape)
#S_trail=np.zeros((1,dimentions))
            

#TODO check if it is true to make Zi as 5 samples for each i and summing them
#or is this the thing that made us say that Si was covariance of sth in the first place? 
S=np.zeros((dimentions,dimentions))
S_initial=np.zeros((nImages_in_each_class // 2,dimentions))
for i in range (number_of_classes):
    for j in range (nImages_in_each_class // 1):
        if (j % 2== 0):    
            S_i=S_initial
            S_i[j // 2]+=Z[i*10+j]
    S_i=np.dot(S_i.T,S_i)
    S+=S_i

#S_inv=np.linalg.inv(S)
#S_inv= np.linalg.solve(S, np.identity(10304))
S_inv= np.linalg.pinv(S)
#Eigen vectors and values:
S_inv_mul_B=np.matmul(S_inv,Sb)
print("shapes of S_inv and Sb,S_inv_mul_",S_inv.shape,Sb.shape,S_inv_mul_B)

#commenting those because of high processing

eigenvals,eigenvecs = np.linalg.eig(S_inv_mul_B)
#sorting eigen values in descending order
sort_idx = np.argsort(eigenvals)[::-1]
eigenvals = eigenvals[sort_idx]
eigenvecs = eigenvecs[:,sort_idx]
print("shape of eigen values",eigenvals)
print("shape of eigen vectors",eigenvecs)

lamb=np.zeros(39)
u=np.zeros((39,10304))
for i in range(39):
    lamb[i]+=eigenvals[i]
    u[i]+=eigenvecs[:,i]
    
#p_train=(u.transpose().dot(train))/(u.transpose().dot(u)) *u    
#p_test=(u.transpose().dot(train))/(u.transpose().dot(u)) *u


    
